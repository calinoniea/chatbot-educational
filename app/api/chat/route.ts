// app/api/chat/route.ts
import { NextResponse } from 'next/server';
import Groq from 'groq-sdk';

// ðŸš¨ CORECTURÄ‚ CRITICÄ‚: Definirea Tipului de Mesaj Ã®n mod manual
// Aceasta rezolvÄƒ eroarea de compilare 'Module not found: ChatCompletionMessageParam'
type GroqMessage = {
  role: 'user' | 'assistant' | 'system';
  content: string;
};

// IniÈ›ializarea clientului Groq
const groq = new Groq({
    apiKey: process.env.GROQ_API_KEY, // CititÄƒ din .env.local sau Vercel
});

// Definirea System Prompt-ului (folosind noul tip)
const systemPrompt: GroqMessage = {
  role: "system", 
  content: "EÈ™ti EduBot, un asistent AI academic È™i profesional, specializat strict pe RomÃ¢nia. Misiunea ta este sÄƒ oferi rÄƒspunsuri precise, obiective È™i documentate despre legislaÈ›ie, istorie, geografie È™i educaÈ›ia din RomÃ¢nia. FoloseÈ™te un ton formal È™i politicos. DacÄƒ o Ã®ntrebare nu are legÄƒturÄƒ cu RomÃ¢nia, refuzÄƒ politicos sÄƒ rÄƒspunzi."
};

export async function POST(req: Request) {
    try {
        const { history } = await req.json();

        if (!history || !Array.isArray(history)) {
            return NextResponse.json({ error: 'Istoric invalid' }, { status: 400 });
        }

        // 1. Adaptarea È™i Maparea Formatului Istoric (Front-End -> Groq)
        // Flowise folosea "userMessage" / "apiMessage"
        // Groq/OpenAI folosesc "user" / "assistant"
        const mappedMessages: GroqMessage[] = history.map((msg: any) => {
            const role = msg.role === 'userMessage' ? 'user' : 'assistant';
            
            return {
                role: role as 'user' | 'assistant', 
                content: msg.content as string, 
            };
        });
        
        // 2. INJECTÄ‚M System Prompt-ul È™i adÄƒugÄƒm mesajele utilizatorului
        const finalMessages: GroqMessage[] = [
            systemPrompt, 
            ...mappedMessages 
        ];

        // 3. ApelÄƒm API-ul Groq
        const chatCompletion = await groq.chat.completions.create({
            messages: finalMessages, 
            // ðŸš¨ CRITIC: AsigurÄƒ-te cÄƒ foloseÈ™ti un model ACTIV, confirmat de tine
            model: "llama-3.1-8b-instant", 
            temperature: 0.7,
        });
        
        // 4. Extragem È™i returnÄƒm rÄƒspunsul
        const botResponse = chatCompletion.choices[0].message.content;

        return NextResponse.json({ text: botResponse }, { status: 200 });

    } catch (error) {
        console.error("Eroare Groq API:", error);
        return NextResponse.json({ error: 'Eroare la procesarea cererii Groq.' }, { status: 500 });
    }
}